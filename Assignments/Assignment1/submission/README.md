# Intro to NLP – Assignment 1 (Tokenisation & LMs)
## Directory Contents
The directory contains the following files:  
* the code for cleaning and tokenisation in `clean_tweets.py`
* the cleaned version of the tweets corpus `2020114001_tokenize.txt`
* the n-gram language model `language_model.py`
* the training and testing corpora `euro-train.txt`, `euro-test.txt`, `medi-train.txt`, `medi-test.txt` (if these are not present, the file will regenerate them randomly)
* the `Perplexities` subdirectory, with the perplexities of the first 1000 of each of the above corpora using the four LMs specified `2020114001_LM<n>_train-perplexity.txt` and `2020114001_LM<n>_test-perplexity.txt`

## Running Functions
### Cleaning Tweets
The `clean_tweets.py` file, when run, opens the corpus file (assumed to be `../corpora/general-tweets.txt`) and writes the cleaned tweets into `2020114001_tokenize.txt`.  
There are two functions: `clean()` and `tokenize()`. If the `-i` flag is passed while running Python3, one can use the REPL to execute these functions:
```py
> python3 -i clean_tweets.py
>>> sentence = "@goodsharing a good close-up of an #f1 steering wheel! on Twitpic http://retwt.me/1KlCs (via @TheFifthDriver)"
>>> clean(sentence)
'<MENTION> a good close-up of an <HASHTAG> steering wheel! on twitpic <URL> (via <MENTION>)'
>>> tokenize(sentence)
['<MENTION>', 'a', 'good', 'close', '-', 'up', 'of', 'an', '<HASHTAG>', 'steering', 'wheel', '!', 'on', 'twitpic', '<URL>', '(', 'via', '<MENTION>', ')']
```

### Language Models
When the `language_model.py` file is run, it uses the command-line arguments (the value of `n`, the smoothing method, and the corpus) to create a language model. It prints a prompt asking for a sentence and then prints the perplexity of the sentence given, according to the language model.
```
> python3 language_model.py 4 k ../corpora/europarl-corpus.txt
input sentence: Thank you.
6.292045341302931e-06
```

The perplexities of further sentences can be found by calling the `lang_mod` function in the REPL (if the program was run with the `-i` flag). Note that `math.exp` must also be called, since the `lang_mod` function returns the log of the true probability.
```py
>>> math.exp(lang_mod("I would like to thank you."))
1.3473665436392813e-06
```

In the REPL, `generate_files()` can also be run. This will generate the four language models specified and run them on the train and test sets of the corpora (1000 sentences each), creating eight files.  

New language models can be generated by calling the `language_model` function and passing the value of `n`, the smoothing method, the path to the corpus, and the filenames of the training and testing sets.

## Explanations
### `clean_tweets.py`
There are two functions: `clean()` and `tokenize()`.  
`clean()` takes a string and returns a cleaned version of it. It takes care of the following cases:  
* hashtags, assumed to be a hash `#` followed by one or more alphabets
* email IDs, assumed to be a string of alphabets and periods, followed by `@`, followed by a string of alphabets, periods and digits
* mentions, assumed to be `@` followed by a string of alphabets, digits, periods and underscores
* numbers, assumed to be strings of digits, optionally followed by commas and more digits, optionally followed by a point and more digits, optionally followed by `%`
* URLs, assumed to be a sequence of alphabets, colons, slashes, followed by a period, a sequence of letters, slashes and digits, optionally followed by more periods and sequences of alphabets, slashes, and digits, optionally followed by more slashes and sequences of alphabets, slashes, digits, and special characters.
* time expressions of the form HH:MM, optionally followed by a space, followed by AM or PM in either case
* money expressions, assumed to be numbers (as described above) preceded by `$`
* repeated punctuation and spaces, which is reduced to a single symbol (e.g. `>>>>>` becomes `>`)
* leading and trailing spaces, which are deleted

`tokenize()` uses `clean()` to carry out tokenisation and return the list of tokens in the string passed to it. It first runs `clean()` on the string and then separates out punctuation, contractions (`'s`, `'m`, `'re`, `'ll`, `'d`, `'ve`, `n't`), and genitive markers (`'s`). The special cases of `can't`, `won't` and `shan't` are hardcoded.  
Then all placeholders are spaced out and the string is split along spaces.

### `language_model.py`
There are important functions in this file: `get_testset()`, `get_frequencies()`, `smooth()`, `language_model()`, and `perplexity()`.  

`get_testset()` takes the corpus filepath, the name of the trainset file and the name of the testset file, and partitions the corpus. It shuffles the lines of the corpus, takes the first 1000 for the testset, and adds the rest to the trainset. It has no return value.  

`get_frequencies()` takes the location of the training set and the value of `n`. It then creates `n` dictionaries, containing the frequencies of `i`-grams for each `i` from 1 to `n`, by iterating over the trainset. It returns these dictionaries and the vocabulary obtained from the trainset.  

`smooth()` takes the smoothing method (`k` for Kneser-Ney and `w` for Witten-Bell), the name of the file containing the trainset, and the value of `n`. It calls `get_frequencies()` and then defines a function that finds the logprobability of `n`-grams passed to it, named `prob()`. This function is the return value.  

`language_model()` behaves like a wrapper. It takes all relevant information – the value of `n`, the smoothing method, the path to the corpus, the trainset filename, and the testset filename – and returns a function from sentences to their logprobabilities. It defines this function as calling the smoothed function on each set of `n` words in the sentence, adding the logprobs together and returning the sum. The sentence passed to it can optionally be pre-tokenised, in which case the second argument needs to be `tok`.  

`perplexity()` only takes a sentence and a language model, and calculates the perplexity of the model on the sentence using the ordinary formula for perplexity.
